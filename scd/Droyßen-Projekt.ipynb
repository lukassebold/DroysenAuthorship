{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314adbee-4528-45b6-a661-f44a7609d52a",
   "metadata": {},
   "source": [
    "# Droyßen Authorship-Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123014d-6969-4bc2-9729-a0ae5e17ae83",
   "metadata": {},
   "source": [
    "## Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c711300-8e23-4fe9-8527-6024194f5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import scipy.spatial.distance as scidist\n",
    "import sklearn.decomposition\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3812cf1-bcec-4bc5-af4b-c226b9483546",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32198ece-955d-49cb-b908-cbe0167fb6b8",
   "metadata": {},
   "source": [
    "### Helferfunktion zum Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2586f8b7-1be5-426d-b9b9-ae1a83e83338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory(directory, max_length):\n",
    "    documents, authors, titles = [], [], [] \n",
    "    for filename in os.scandir(directory):\n",
    "        if not filename.name.endswith('.txt'):\n",
    "            continue\n",
    "        author, _ = os.path.splitext(filename.name)\n",
    "\n",
    "        with open(filename.path) as f:\n",
    "            contents = f.read()\n",
    "        lemmas = contents.lower().split()\n",
    "        start_idx, end_idx, segm_cnt = 0, max_length, 1\n",
    "\n",
    "        # extract slices from the text:\n",
    "        while end_idx < len(lemmas):\n",
    "            documents.append(' '.join(lemmas[start_idx:end_idx]))\n",
    "            authors.append(author[0])\n",
    "            title = filename.name.replace('.txt', '').split('_')[1]\n",
    "            titles.append(f\"{title}-{segm_cnt}\")\n",
    "\n",
    "            start_idx += max_length\n",
    "            end_idx += max_length\n",
    "            segm_cnt += 1\n",
    "\n",
    "    return documents, authors, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914e3ae-d8ab-4f75-8e0c-01694cd6a407",
   "metadata": {},
   "source": [
    "Vokabular: verschiedene Listen: Ausgangspunkt: most frequent words, dann schrittweise neue Listen erstellen mit Wörtern, die man rausnimmt (mit Begründung im Text), dann über Code von Ursprungsliste \"abziehen\"\n",
    "Anders als Kestemon: dort war die Liste mit \"##\" aussortiert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b6412-f51c-4282-a045-9c608f045070",
   "metadata": {},
   "source": [
    "## Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e93f02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9772/4248975668.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/texts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_directory' is not defined"
     ]
    }
   ],
   "source": [
    "documents, authors, titles = load_directory('data/texts', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8b144",
   "metadata": {},
   "source": [
    "## Erstelle Vokabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [l.strip() for l in open('data/wordlist.txt') if not l.startswith('#') and l.strip()][:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiieren\n",
    "vectorizer = text.CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", vocabulary=vocab)\n",
    "\n",
    "# Fit und Transform\n",
    "v_documents = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Check\n",
    "print(v_documents.shape)\n",
    "print(vectorizer.get_feature_names_out()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisiere\n",
    "n_v_documents = preprocessing.normalize(v_documents.astype(float), norm='l1')\n",
    "\n",
    "# Check\n",
    "print(n_v_documents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "s_documents = scaler.fit_transform(n_v_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47227d5",
   "metadata": {},
   "source": [
    "## Helferfunktion für Plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c82a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_scores(nchunk=0):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16,6))\n",
    "\n",
    "    labels = vectorizer.get_feature_names_out()\n",
    "    x = np.arange(0,65)\n",
    "\n",
    "    ax.bar(x=x, height=s_documents[nchunk])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=90, fontsize=12)\n",
    "    ax.set_title(f'Textchunk {authors[nchunk]}_{titles[nchunk]}')\n",
    "    ax.set_xlabel('Features', fontsize=14)\n",
    "    ax.set_ylabel('z-Werte', fontsize=14)\n",
    "    ax.set_ylim(-3.5, 3.5)\n",
    "    ax.yaxis.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielplot für ersten Chunk, als Test\n",
    "plot_z_scores(nchunk=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe0e67",
   "metadata": {},
   "source": [
    "## Cityblock-Distanzen zu allen Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa7c9f",
   "metadata": {},
   "source": [
    "test_doc = s_documents[0]\n",
    "\n",
    "distances = [ scidist.cityblock(test_doc, train_doc) for train_doc in s_documents[1:] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for distance, author, title in zip(distances, authors[1:], titles[1:]):\n",
    "    print(f'{distance} => {author}_{title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869af375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'geringste Cityblock-Distanz: {distances[np.argmin(distances)]} => {authors[np.argmin(distances) + 1]}_{titles[np.argmin(distances) + 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56e2b0",
   "metadata": {},
   "source": [
    "### Plotte Testchunk und die jeweils nächsten und entferntesten Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c70a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_z_scores(nchunk=0)                         # Testchunk\n",
    "plot_z_scores(nchunk=np.argmin(distances) + 1)  # nächster Chunk\n",
    "plot_z_scores(nchunk=np.argmax(distances) + 1)  # entfernster Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b52a6",
   "metadata": {},
   "source": [
    "## Delta-Objekt erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Delta:\n",
    "    \"\"\"Delta-Based Authorship Attributer.\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit (or train) the attributer.\n",
    "\n",
    "        Arguments:\n",
    "            X: a two-dimensional array of size NxV, where N represents\n",
    "               the number of training documents, and V represents the\n",
    "               number of features used.\n",
    "            y: a list (or NumPy array) consisting of the observed author\n",
    "                for each document in X.\n",
    "\n",
    "        Returns:\n",
    "            Delta: A trained (fitted) instance of Delta.\n",
    "\n",
    "        \"\"\"\n",
    "        self.train_y = np.array(y)\n",
    "        self.scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "        self.train_X = self.scaler.fit_transform(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, metric='cityblock'):\n",
    "        \"\"\"Predict the authorship for each document in X.\n",
    "\n",
    "        Arguments:\n",
    "            X: a two-dimensional (sparse) matrix of size NxV, where N\n",
    "               represents the number of test documents, and V represents\n",
    "               the number of features used during the fitting stage of\n",
    "               the attributer.\n",
    "            metric (str, optional): the metric used for computing\n",
    "               distances between documents. Defaults to 'cityblock'.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: the predicted author for each document in X.\n",
    "\n",
    "        \"\"\"\n",
    "        X = self.scaler.transform(X)\n",
    "        dists = scidist.cdist(X, self.train_X, metric=metric)\n",
    "        return self.train_y[np.argmin(dists, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ba4c2",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(set(authors)) * 2\n",
    "\n",
    "(train_documents, test_documents, train_authors, test_authors) = model_selection.train_test_split(n_v_documents,  # normalisierte Daten werden genutzt\n",
    "                                                                                                 authors, \n",
    "                                                                                                 test_size=test_size, \n",
    "                                                                                                 stratify=authors, \n",
    "                                                                                                 random_state=42)\n",
    "                                                                                                 \n",
    "print(f'N={test_documents.shape[0]} Test Dokumente mit '\n",
    "      f'V={test_documents.shape[1]} Features.')\n",
    "\n",
    "print(f'N={train_documents.shape[0]} Training Dokumente mit '\n",
    "      f'V={train_documents.shape[1]} Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a211bf02",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = Delta()                             # Delta Classifier wird instantiiert\n",
    "delta.fit(train_documents, train_authors)   # Delta Classifier wird gefitted\n",
    "preds = delta.predict(test_documents)       # Delta Classifier klassifziert Test Dokumente\n",
    "\n",
    "# Ausgabe\n",
    "for true, pred in zip(test_authors, preds):\n",
    "    _connector = 'ABER' if true != pred else 'und'\n",
    "    print(f'Der Autor ist {true} {_connector} {pred} wurde vorhergesagt.')\n",
    "\n",
    "accuracy = metrics.accuracy_score(preds, test_authors)\n",
    "print(f'\\nAccuracy der Vorhersagen: {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a4b7a",
   "metadata": {},
   "source": [
    "# Anwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17317a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Trainingsdaten\n",
    "train_documents, train_authors, train_titles = load_directory('data/texts', 3301)\n",
    "\n",
    "vectorizer = text.CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", vocabulary=vocab)\n",
    "                                  \n",
    "v_train_documents = vectorizer.fit_transform(train_documents).toarray()\n",
    "v_train_documents = preprocessing.normalize(v_train_documents.astype(float), norm='l1')\n",
    "\n",
    "delta = Delta().fit(v_train_documents, train_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363bff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Testdaten\n",
    "test_docs, test_authors, test_titles = load_directory('data/texts/test', 3301)\n",
    "\n",
    "v_test_docs = vectorizer.transform(test_docs).toarray()\n",
    "v_test_docs = preprocessing.normalize(v_test_docs.astype(float), norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b45dff",
   "metadata": {},
   "source": [
    "## Klassifizieren mit Cityblock-Distanz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024838d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = delta.predict(v_test_docs)\n",
    "\n",
    "for test_author, test_title, prediction in zip(test_authors, test_titles, predictions):\n",
    "    print(f'Quelle: {test_author}_{test_title} => klassifiziert als {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1335a209088fba08ed1ec7ee6e9c6b845cf55524ee893ab338ddd70a4ed03024"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
